Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
    x·ª≠ l√Ω khuy·∫øt thi·∫øu
    m√£ h√≥a d·ªØ li·ªáu (encoding)
    chu·∫©n h√≥a d·ªØ li·ªáu (normalization/scaling)
    th·ªëng k√™ m√¥ t·∫£
    tr·ª±c quan h√≥a d·ªØ li·ªáu
        bi·ªÉu ƒë·ªì histogram, scatter, boxplot
    ph√¢n t√≠ch t∆∞∆°ng quan 

Kh√°m ph√° & tr·ª±c quan h√≥a d·ªØ li·ªáu (EDA - Exploratory Data Analysis)
    M·ª•c ti√™u
        Hi·ªÉu c·∫•u tr√∫c, ph√¢n ph·ªëi, m·ªëi quan h·ªá v√† b·∫•t th∆∞·ªùng trong d·ªØ li·ªáu.
    Checklist k·ªπ thu·∫≠t
        Ki·ªÉm tra k√≠ch th∆∞·ªõc, ki·ªÉu d·ªØ li·ªáu.
        Th·ªëng k√™ m√¥ t·∫£ (mean, median, std).
        X√°c ƒë·ªãnh outlier.
        Ph√¢n t√≠ch t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn.
        Tr·ª±c quan h√≥a (hist, boxplot, heatmap).
    C√¥ng c·ª•/Th∆∞ vi·ªán
        pandas, matplotlib, seaborn, ydata-profiling

Hu·∫•n luy·ªán m√¥ h√¨nh (Model Training)
    M·ª•c ti√™u
        X√¢y d·ª±ng m√¥ h√¨nh ML ph√π h·ª£p v√† hu·∫•n luy·ªán tr√™n d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω.
    Checklist k·ªπ thu·∫≠t
        Ch·ªçn thu·∫≠t to√°n ph√π h·ª£p (SVM, Random Forest, XGBoost, v.v.).
        Thi·∫øt l·∫≠p pipeline hu·∫•n luy·ªán.
        Ch·∫°y hu·∫•n luy·ªán v√† l∆∞u model.
        Theo d√µi loss/metric.
        Ki·ªÉm tra overfitting/underfitting.
    C√¥ng c·ª•/Th∆∞ vi·ªán
        scikit-learn, xgboost, lightgbm, tensorflow, pytorch

Fine-tuning m√¥ h√¨nh (Model Optimization)
    M·ª•c ti√™u
        T·ªëi ∆∞u si√™u tham s·ªë ƒë·ªÉ ƒë·∫°t hi·ªáu nƒÉng cao nh·∫•t.
    Checklist k·ªπ thu·∫≠t
        Ch·ªçn tham s·ªë c·∫ßn t·ªëi ∆∞u.
        D√πng GridSearchCV / RandomizedSearchCV / Optuna.
        ƒê√°nh gi√° b·∫±ng cross-validation.
        Theo d√µi metric trung b√¨nh v√† ƒë·ªô l·ªách chu·∫©n.
        L∆∞u k·∫øt qu·∫£ t·ªëi ∆∞u.
    C√¥ng c·ª•/Th∆∞ vi·ªán    
        scikit-learn, optuna, ray[tune]        

V·∫≠n h√†nh & b·∫£o d∆∞·ª°ng (Deployment & Maintenance)
    M·ª•c ti√™u
        ƒê∆∞a m√¥ h√¨nh v√†o m√¥i tr∆∞·ªùng s·∫£n xu·∫•t, gi√°m s√°t v√† c·∫≠p nh·∫≠t ƒë·ªãnh k·ª≥.
        ƒë√≥ng h√≥i m√¥ t√¨nh 
        tri·ªÉn khai 
        gi√°m s√°t 
        drift (tr√¥i d·∫°t d·ªØ li·ªáu)
        hi·ªáu su·∫•t
        t√°i hu·∫•n luy·ªán 
    Checklist k·ªπ thu·∫≠t
        L∆∞u model (joblib, onnx, pickle).
        Tri·ªÉn khai API b·∫±ng FastAPI ho·∫∑c Flask.
        Theo d√µi performance th·ª±c t·∫ø (drift detection).
        C·∫≠p nh·∫≠t model ƒë·ªãnh k·ª≥.
        Qu·∫£n l√Ω version v√† logs.
        B·∫£o m·∫≠t v√† ki·ªÉm so√°t truy c·∫≠p.
    C√¥ng c·ª•/Th∆∞ vi·ªán
        FastAPI, MLflow, Docker, Prometheus, Grafana

---
```python
from sklearn import datasets
digits = datasets.load_digits()
feature = digits.data
target = digits.target
feature[0]
```
### üß† Gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc
| D√≤ng                              | √ù nghƒ©a                                                 | Ghi ch√∫ k·ªπ thu·∫≠t                                                       |
| --------------------------------- | ------------------------------------------------------- | ---------------------------------------------------------------------- |
| `from sklearn import datasets`    | Import module `datasets` c·ªßa scikit-learn               | Th∆∞ vi·ªán n√†y ch·ª©a nhi·ªÅu b·ªô d·ªØ li·ªáu m·∫´u (Iris, Wine, Digits, Boston...) |
| `digits = datasets.load_digits()` | T·∫£i **b·ªô d·ªØ li·ªáu handwritten digits (ch·ªØ s·ªë vi·∫øt tay)** | D·ªØ li·ªáu g·ªìm 1.797 ·∫£nh 8√ó8 pixel c·ªßa c√°c ch·ªØ s·ªë 0‚Äì9                     |
| `feature = digits.data`           | L·∫•y **ma tr·∫≠n ƒë·∫∑c tr∆∞ng (features)**                    | M·ªói ·∫£nh ƒë∆∞·ª£c flatten th√†nh vector 64 gi√° tr·ªã (8√ó8 pixel ‚Üí 64 features) |
| `target = digits.target`          | L·∫•y **nh√£n (labels)** t∆∞∆°ng ·ª©ng                         | Nh√£n l√† s·ªë nguy√™n t·ª´ 0 ƒë·∫øn 9                                           |
| `feature[0]`                      | Xem **m·∫´u ƒë·∫ßu ti√™n** trong d·ªØ li·ªáu                      | In ra 64 gi√° tr·ªã pixel (d·∫°ng s·ªë th·ª±c t·ª´ 0‚Äì16)                          |

---
### V√≠ d·ª• minh h·ªça

```python
import matplotlib.pyplot as plt

plt.imshow(digits.images[0], cmap='gray')
plt.title(f'Label: {digits.target[0]}')
plt.show()
```
L·ªánh n√†y s·∫Ω hi·ªÉn th·ªã **·∫£nh ch·ªØ s·ªë vi·∫øt tay ƒë·∫ßu ti√™n** trong t·∫≠p d·ªØ li·ªáu (v√≠ d·ª•: ‚Äú0‚Äù ho·∫∑c ‚Äú3‚Äù).

---
### Th√¥ng tin nhanh

* `digits.data.shape` ‚Üí `(1797, 64)`
  ‚Üí C√≥ 1.797 m·∫´u, m·ªói m·∫´u c√≥ 64 ƒë·∫∑c tr∆∞ng.
* `digits.target_names` ‚Üí `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`

---

### ·ª®ng d·ª•ng

B·ªô **Digits dataset** n√†y th∆∞·ªùng d√πng ƒë·ªÉ:

* Th·ª≠ nghi·ªám nhanh c√°c thu·∫≠t to√°n ph√¢n lo·∫°i (SVM, RandomForest, LogisticRegression).
* D·∫°y kh√°i ni·ªám v·ªÅ pipeline ML.
* Ki·ªÉm th·ª≠ m√¥ h√¨nh tr∆∞·ªõc khi √°p d·ª•ng v√†o d·ªØ li·ªáu th·∫≠t.



k·∫øt h·ª£p dataframes
    n·ªëi
        v·∫•n ƒë·ªÅ: "x·∫øp ch·ªìng" c√°c dataframe
        pd.concat([df_a,df_b], axis=0)(n·ªëi theo h√†ng)
        pd.concat([df_a,df_b], axis=1)(n·ªëi theo c·ªôt)
    tr·ªôn
        v·∫•n ƒë·ªÅ join c√°c dataframe gi·ªëng nh∆∞ trong sql

c√°c k·ªπ thu·∫≠t co d√£n


| Giai ƒëo·∫°n                  | Tr·∫°ng th√°i | N·ªôi dung ch√≠nh             |
| -------------------------- | ---------- | -------------------------- |
| 1Ô∏è‚É£ Xem x√©t b√†i to√°n       | ‚úÖ          | Ch·ªçn d·∫°ng h·ªìi quy          |
| 2Ô∏è‚É£ T·∫°o/L·∫•y d·ªØ li·ªáu        | ‚úÖ          | D√πng `make_regression()`   |
| 3Ô∏è‚É£ Kh√°m ph√° d·ªØ li·ªáu (EDA) | üîú         | Ki·ªÉm tra, tr·ª±c quan h√≥a    |
| 4Ô∏è‚É£ Ti·ªÅn x·ª≠ l√Ω             | ‚è≠Ô∏è         | Chu·∫©n h√≥a, chia train/test |
| 5Ô∏è‚É£ Hu·∫•n luy·ªán m√¥ h√¨nh     | ...        | `LinearRegression.fit()`   |
| 6Ô∏è‚É£ Fine-tuning            | ...        | ƒêi·ªÅu ch·ªânh hyperparameter  |
| 7Ô∏è‚É£ V·∫≠n h√†nh & b·∫£o d∆∞·ª°ng   | ...        | Deploy v√† gi√°m s√°t model   |


d·ªØ li·ªáu th√¥
d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
file pyinb


file d·ªØ li·ªáu th√¥
file d·ªØ li·ªáu ƒë·ªÉ d√πng ƒë·ªÉ hu·∫•n luy·ªán
file ipynb (2 code cell cu·ªëi ƒë·ªÉ show 30 d√≤ng c·ªßa t·∫≠p th√¥ v√† 30 d√≤ng c·ªßa t·∫≠p ƒë√£ x·ª≠ l√Ω)

NUM_EPOCHS = 100

# Initialize random weights (Kh·ªüi t·∫°o tr·ªçng s·ªë ng·∫´u nhi√™n)
# W: Tr·ªçng s·ªë (Slope), b: H·ªá s·ªë t·ª± do (Bias/Intercept)
W = 0.01 * np.random.randn(INPUT_DIM, OUTPUT_DIM)
b = np.zeros((1, ))

# Training loop (V√≤ng l·∫∑p hu·∫•n luy·ªán)
for epoch_num in range(NUM_EPOCHS):

    # Forward pass [NX1] . [1X1] = [NX1] (Lan truy·ªÅn xu√¥i)
    # T√≠nh gi√° tr·ªã d·ª± ƒëo√°n y_pred = X * W + b
    y_pred = np.dot(X_train, W) + b

    # Loss (H√†m m·∫•t m√°t - Mean Squared Error)
    # T√≠nh sai s·ªë trung b√¨nh b√¨nh ph∆∞∆°ng
    loss = (1/len(y_train)) * np.sum((y_train - y_pred)**2)

    # Show progress (Hi·ªÉn th·ªã ti·∫øn tr√¨nh)
    if epoch_num % 10 == 0:
        print(f"Epoch: {epoch_num}, loss: {loss:.3f}")

    # Backpropagation (Lan truy·ªÅn ng∆∞·ª£c)
    # T√≠nh ƒë·∫°o h√†m (gradient) ƒë·ªÉ bi·∫øt h∆∞·ªõng ƒëi·ªÅu ch·ªânh W v√† b
    # L∆∞u √Ω: Trong h√¨nh d√πng bi·∫øn N, gi·∫£ ƒë·ªãnh N = len(y_train)
    dW = -(2/N) * np.sum((y_train - y_pred) * X_train)
    db = -(2/N) * np.sum((y_train - y_pred) * 1)

    # Update weights (C·∫≠p nh·∫≠t tr·ªçng s·ªë)
    # D√πng thu·∫≠t to√°n Gradient Descent ƒë·ªÉ t·ªëi ∆∞u h√≥a
    W += -LEARNING_RATE * dW
    b += -LEARNING_RATE * db


    import numpy as np
import pandas as pd
data = pd.read_csv("housing_processed.csv")
data = data.iloc[:100]
X = data.drop(columns=['median_house_value']).values
y = data[['median_house_value']].values
nsample = 100
train = int(0.8 * nsample)
test= nsample - train
X_train = X[:train]
y_train = y[:train]

X_test = X[train:]
y_test = y[train:]
n_features = X_train.shape[1]

w = np.zeros((n_features, 1))
b = 0.0
LR = 0.01
nepoch = 50
def mse(y_true, y_pred):
¬† ¬† return np.mean((y_true - y_pred)**2)
for epoch in range(nepoch):
¬† ¬† y_pred = X_train.dot(w) + b
¬† ¬† loss = mse(y_train, y_pred)
¬† ¬† dw = (-2/train) * X_train.T.dot(y_train - y_pred)
¬† ¬† db = (-2/train) * np.sum(y_train - y_pred)
¬† ¬† w = w - LR * dw
¬† ¬† b = b - LR * db

¬† ¬† print(f"Epoch {epoch+1}/{nepoch}, Loss = {loss}")
y_pred_train = X_train.dot(w) + b
y_pred_test = X_test.dot(w) + b

MSE_train = mse(y_train, y_pred_train)
MSE_test = mse(y_test, y_pred_test)
print("w shape =", w.shape)
print("b =", b)
print("MSE Train =", MSE_train)
print("MSE Test ¬†=", MSE_test)

output
Epoch 1/50, Loss = 0.09393447602140333
Epoch 2/50, Loss = 0.08371964502882284
Epoch 3/50, Loss = 0.0748873950430457
Epoch 4/50, Loss = 0.0672504382125044
Epoch 5/50, Loss = 0.06064685734908866
Epoch 6/50, Loss = 0.054936669129589645
Epoch 7/50, Loss = 0.04999885285786364
Epoch 8/50, Loss = 0.045728781721236046
Epoch 9/50, Loss = 0.04203600201787216
Epoch 10/50, Loss = 0.03884231321775387
Epoch 11/50, Loss = 0.03608010810529291
Epoch 12/50, Loss = 0.03369093777201489
Epoch 13/50, Loss = 0.03162427100033936
Epoch 14/50, Loss = 0.029836421705560147
Epoch 15/50, Loss = 0.02828962167027573
Epoch 16/50, Loss = 0.02695121888944797
Epoch 17/50, Loss = 0.025792984510435026
Epoch 18/50, Loss = 0.024790513657344783
Epoch 19/50, Loss = 0.023922707421811094
Epoch 20/50, Loss = 0.023171325025106215
Epoch 21/50, Loss = 0.022520596645935403
Epoch 22/50, Loss = 0.0219568886959289
Epoch 23/50, Loss = 0.021468414438083148
Epoch 24/50, Loss = 0.021044983805837296
Epoch 25/50, Loss = 0.02067778711253018
Epoch 26/50, Loss = 0.020359208060328565
Epoch 27/50, Loss = 0.02008266207961826
Epoch 28/50, Loss = 0.019842456567505027
Epoch 29/50, Loss = 0.01963367005889568
Epoch 30/50, Loss = 0.01945204776548596
Epoch 31/50, Loss = 0.01929391126540137
Epoch 32/50, Loss = 0.019156080426594353
Epoch 33/50, Loss = 0.01903580590677026
Epoch 34/50, Loss = 0.018930710797108896
Epoch 35/50, Loss = 0.01883874017113098
Epoch 36/50, Loss = 0.01875811746785115
Epoch 37/50, Loss = 0.01868730678342115
Epoch 38/50, Loss = 0.0186249802708786
Epoch 39/50, Loss = 0.018569989956039416
Epoch 40/50, Loss = 0.018521343371307793
Epoch 41/50, Loss = 0.018478182490215222
Epoch 42/50, Loss = 0.018439765515560362
Epoch 43/50, Loss = 0.018405451134590953
Epoch 44/50, Loss = 0.018374684907033704
Epoch 45/50, Loss = 0.01834698749704912
Epoch 46/50, Loss = 0.0183219444993268
Epoch 47/50, Loss = 0.01829919764337326
Epoch 48/50, Loss = 0.018278437189297708
Epoch 49/50, Loss = 0.018259395353691057
Epoch 50/50, Loss = 0.01824184062605842
w shape = (12, 1)
b = 0.07378415672497833
MSE Train = 0.018225572855167384
MSE Test  = 0.030460584386512852

import pandas as pd

data = pd.read_csv("housing_processed.csv")
print(data.columns)
Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',
       'total_bedrooms', 'population', 'households', 'median_income',
       'median_house_value', 'ocean_proximity_INLAND',
       'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',
       'ocean_proximity_NEAR OCEAN'],
      dtype='object')

print(data.head())
   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \
0   0.213996  0.564293                 1.0     0.257379        0.162209   
1   0.212982  0.564293                 1.0     0.223472        0.201035   
2   0.212982  0.564293                 1.0     0.285488        0.239862   
3   0.212982  0.564293                 1.0     0.161103        0.182053   
4   0.212982  0.563231                 1.0     0.445011        0.420190   

   population  households  median_income  median_house_value  \
0    0.157609    0.160550       0.899633            0.721533   
1    0.177430    0.199083       0.684719            0.698417   
2    0.179668    0.235780       0.445496            0.700343   
3    0.131074    0.175229       0.470871            0.545164   
4    0.348785    0.469725       0.420587            0.608306   

   ocean_proximity_INLAND  ocean_proximity_ISLAND  ocean_proximity_NEAR BAY  \
0                     0.0                     0.0                       1.0   
1                     0.0                     0.0                       1.0   
2                     0.0                     0.0                       1.0   
3                     0.0                     0.0                       1.0   
4                     0.0                     0.0                       1.0   

   ocean_proximity_NEAR OCEAN  
0                         0.0  
1                         0.0  
2                         0.0  
3                         0.0  
4                         0.0  



## 1. **M√¥ h√¨nh d·ª± ƒëo√°n**

### **1.1. Z ƒë·∫ßu ra tuy·∫øn t√≠nh**

[
z_i = W x_i + b
]

* ( x_i ): vector ƒë·∫∑c tr∆∞ng c·ªßa m·∫´u th·ª© i (D chi·ªÅu)
* ( W ): ma tr·∫≠n tr·ªçng s·ªë k√≠ch th∆∞·ªõc ((K \times D))
* ( b ): vector bias ((K \times 1))
* ( z_i ): ƒë·∫ßu ra tuy·∫øn t√≠nh tr∆∞·ªõc softmax (K chi·ªÅu)

---

### **1.2. H√†m softmax**

[
\hat{y}*{ij} = \frac{e^{z*{ij}}}{\sum_{k=1}^{K} e^{z_{ik}}}
]

* ƒê√¢y l√† x√°c su·∫•t m·∫´u th·ª© i thu·ªôc l·ªõp j
* Softmax bi·∫øn vector ( z_i ) th√†nh m·ªôt ph√¢n ph·ªëi x√°c su·∫•t (t·ªïng = 1)

---

## 2. **H√†m m·∫•t m√°t ‚Äì Cross entropy**

[
L_i = - \sum_{j=1}^{K} y_{ij} \log(\hat{y}_{ij})
]

* ( y_{ij} = 1 ) n·∫øu m·∫´u thu·ªôc l·ªõp j, ng∆∞·ª£c l·∫°i 0
  ‚Üí one-hot vector

H√†m n√†y ƒëo ƒë·ªô sai l·ªách gi·ªØa ph√¢n ph·ªëi th·∫≠t v√† ph√¢n ph·ªëi d·ª± ƒëo√°n.

---

## 3. **ƒê·∫°o h√†m (Gradient) ƒë·ªÉ c·∫≠p nh·∫≠t W v√† b**

Ta c·∫ßn t√≠nh:

[
\frac{\partial L_i}{\partial W},\quad \frac{\partial L_i}{\partial b}
]

Hai m·ª•c ti√™u ƒë∆∞·ª£c tr√¨nh b√†y trong ·∫£nh:

---

### ‚≠ê **M·ª•c ti√™u 1: ƒê·∫°o h√†m theo W**

K·∫øt qu·∫£:

[
\frac{\partial L_i}{\partial W} = (\hat{y}_i - y_i) x_i^T
]

Gi·∫£i th√≠ch:

* (\hat{y}_i - y_i) l√† vector k√≠ch th∆∞·ªõc (K√ó1)
* (x_i^T) l√† vector (1√óD)
  ‚Üí Nh√¢n v√†o ra ma tr·∫≠n (K√óD) ƒë√∫ng b·∫±ng k√≠ch th∆∞·ªõc W.

V·ªõi to√†n b·ªô batch N m·∫´u:

[
\frac{\partial L}{\partial W} = \frac{1}{N} \sum_{i=1}^N (\hat{y}_i - y_i) x_i^T
]

---

### ‚≠ê **M·ª•c ti√™u 2: ƒê·∫°o h√†m theo b**

[
\frac{\partial L_i}{\partial b} = (\hat{y}_i - y_i)
]

V·ªõi to√†n b·ªô N m·∫´u:

[
\frac{\partial L}{\partial b} = \frac{1}{N} \sum_{i=1}^N (\hat{y}_i - y_i)
]

---

## 4. **Quy tr√¨nh hu·∫•n luy·ªán (Training Procedure)**

·∫¢nh t√≥m t·∫Øt 3 b∆∞·ªõc:

---

### **B∆∞·ªõc 1 ‚Äî Kh·ªüi t·∫°o**

* Kh·ªüi t·∫°o

  * (W): ma tr·∫≠n ((K \times D))
  * (b): vector ((K \times 1))
* Learning rate: (\eta)

---

### **B∆∞·ªõc 2 ‚Äî Duy·ªát qua t·ª´ng epoch v√† t·ª´ng m·∫´u**

ƒê·ªëi v·ªõi m·ªói m·∫´u:

#### (a) **T√≠nh d·ª± ƒëo√°n**

* T√≠nh (z_i = W x_i + b)
* T√≠nh softmax: (\hat{y}_i)

#### (b) **T√≠nh loss**

[
L_i = -\sum_j y_{ij} \log(\hat{y}_{ij})
]

#### (c) **T√≠nh ƒë·∫°o h√†m**

[
\frac{\partial L_i}{\partial W} = (\hat{y}_i - y_i)x_i^T
]
[
\frac{\partial L_i}{\partial b} = (\hat{y}_i - y_i)
]

#### (d) **C·∫≠p nh·∫≠t tham s·ªë**

[
W = W - \eta \frac{\partial L_i}{\partial W}
]
[
b = b - \eta \frac{\partial L_i}{\partial b}
]

---

### **B∆∞·ªõc 3 ‚Äî K·∫øt th√∫c**

Sau nhi·ªÅu epoch, thu ƒë∆∞·ª£c W v√† b t·ªëi ∆∞u.

---

# üéØ **√ù nghƒ©a to√†n b·ªô n·ªôi dung**

·∫¢nh m√¥ t·∫£ ƒë·∫ßy ƒë·ªß c√°ch hu·∫•n luy·ªán m·ªôt **m√¥ h√¨nh ph√¢n lo·∫°i nhi·ªÅu l·ªõp (softmax classifier)**:

* D√πng **h√†m softmax** ƒë·ªÉ bi·∫øn output th√†nh x√°c su·∫•t.
* D√πng **cross entropy loss** ƒë·ªÉ ƒëo ƒë·ªô l·ªách.
* D√πng **gradient descent** ƒë·ªÉ c·∫≠p nh·∫≠t tr·ªçng s·ªë W, b.
* C√°ch t√≠nh gradient ch√≠nh x√°c:

  * ƒê·∫°o h√†m W = outer product gi·ªØa sai s·ªë v√† vector input
  * ƒê·∫°o h√†m b = sai s·ªë tr·ª±c ti·∫øp
* Chu tr√¨nh l·∫∑p l·∫°i ƒë·∫øn khi t·ªëi ∆∞u.

ƒê√¢y ch√≠nh l√† m√¥ h√¨nh **multiclass logistic regression** ho·∫∑c **output layer c·ªßa neural network**.

l√†m tay c√°i ƒë√≥ v·ªõi l√†m code one sample theo ƒëa bi·∫øn v·ªõi ƒë∆°n bi·∫øn full sample ƒëa v·ªõi ƒë∆°n





1. **Gi·∫£i tay (t√≠nh to√°n gradient + c·∫≠p nh·∫≠t W, b) theo *one-sample***
2. **Gi·∫£i tay theo *full-sample***
3. **Code Python ƒë·∫ßy ƒë·ªß**, g·ªìm:

   * One-sample (online SGD)
   * Full-sample (batch gradient descent)
   * B·∫£n ƒë∆°n bi·∫øn (1 input)
   * B·∫£n ƒëa bi·∫øn (multi-feature)

---

# ‚≠ê **B√ÄI TO√ÅN**

D·ª± ƒëo√°n gi·ªëng hoa (0,1,2) d·ª±a tr√™n **chi·ªÅu r·ªông c√°nh hoa (1 feature)**.
C√≥ 6 m·∫´u:

| x (chi·ªÅu r·ªông) | y (l·ªõp) |
| -------------- | ------- |
| 1.0            | 0       |
| 2.5            | 0       |
| 4.0            | 1       |
| 5.5            | 1       |
| 7.0            | 2       |
| 8.0            | 2       |

D·ª± ƒëo√°n cho x = 3.5 sau khi train.

---

## üéØ Tham s·ªë ban ƒë·∫ßu

Ta c√≥ 3 l·ªõp ‚Üí W k√≠ch th∆∞·ªõc 3√ó1, b k√≠ch th∆∞·ªõc 3√ó1

[
W =
\begin{bmatrix}
0\ 0\ 0
\end{bmatrix},
\quad
b =
\begin{bmatrix}
0\ 0\ 0
\end{bmatrix}
]

Learning rate:
[
\eta = 0.2
]
S·ªë epoch:
[
n_epoch = 2
]

---

# PH·∫¶N 1 ‚Äî ‚≠ê GI·∫¢I TAY ONE-SAMPLE (SGD)

·ªû ƒë√¢y ta c·∫≠p nh·∫≠t **sau t·ª´ng ƒëi·ªÉm d·ªØ li·ªáu**.

---

# üìå **EPOCH 1 ‚Äì SAMPLE 1**

### M·∫´u:

x = 1.0 , y = 0 ‚Üí one-hot = [1,0,0]

### 1) T√≠nh z

[
z = Wx + b =
\begin{bmatrix}0 \ 0 \ 0\end{bmatrix}
]

### 2) Softmax

[
\hat{y} = [1/3,;1/3,;1/3]
]

### 3) Sai s·ªë

[
\hat{y}-y =
\begin{bmatrix}
1/3 - 1 \
1/3 - 0 \
1/3 - 0
\end{bmatrix}
=============

\begin{bmatrix}
-2/3 \
1/3 \
1/3
\end{bmatrix}
]

### 4) Gradient W

[
\frac{\partial L}{\partial W}
=============================

# (\hat{y}-y)x^T

\begin{bmatrix}
-2/3 \
1/3 \
1/3
\end{bmatrix} (1)
]

### 5) C·∫≠p nh·∫≠t W

[
W = W - \eta \frac{\partial L}{\partial W}
]

[
W =
\begin{bmatrix}
0\0\0
\end{bmatrix}

* 0.2
  \begin{bmatrix}
  -2/3\1/3\1/3
  \end{bmatrix}
  =
  \begin{bmatrix}
  0.1333\ -0.0666\ -0.0666
  \end{bmatrix}
  ]

### 6) C·∫≠p nh·∫≠t b (gi·ªëng gradient W nh∆∞ng kh√¥ng nh√¢n x)

[
b = b - 0.2
\begin{bmatrix}
-2/3\1/3\1/3
\end{bmatrix}
=============

\begin{bmatrix}
0.1333\ -0.0666\ -0.0666
\end{bmatrix}
]

---

‚û°Ô∏è **B·∫°n s·∫Ω l·∫∑p l·∫°i t∆∞∆°ng t·ª± cho to√†n b·ªô 6 m·∫´u √ó 2 epoch**.
Do gi·ªõi h·∫°n tin nh·∫Øn, n·∫øu b·∫°n mu·ªën **t√¥i s·∫Ω t√≠nh h·∫øt to√†n b·ªô b·∫£ng 12 update cho b·∫°n**.

---

# PH·∫¶N 2 ‚Äî ‚≠ê GI·∫¢I TAY FULL-SAMPLE (batch)

Full sample nghƒ©a l√†:

1. T√≠nh softmax cho **t·∫•t c·∫£** 6 ƒëi·ªÉm
2. T√≠nh trung b√¨nh gradient
3. Ch·ªâ c·∫≠p nh·∫≠t W, b **m·ªôt l·∫ßn m·ªói epoch**

---

## üéØ C√¥ng th·ª©c

[
\frac{\partial L}{\partial W}
= \frac{1}{N}\sum_{i=1}^N (\hat{y}_i-y_i)x_i^T
]

[
\frac{\partial L}{\partial b}
= \frac{1}{N}\sum_{i=1}^N (\hat{y}_i-y_i)
]

---

### ‚ö†Ô∏è V√¨ ban ƒë·∫ßu W=0, b=0 n√™n t·∫•t c·∫£ m·∫´u ƒë·ªÅu cho c√πng softmax:

[
\hat{y}_i = [1/3,1/3,1/3]
]

### T√≠nh ( \hat{y}-y ) cho t·ª´ng m·∫´u, trung b√¨nh l·∫°i

### (T√¥i s·∫Ω l√†m nhanh, n·∫øu c·∫ßn b·∫°n c√≥ th·ªÉ y√™u c·∫ßu chi ti·∫øt):

[
\sum(\hat{y}-y) =
\begin{bmatrix}
-2 \ -0 \ +2
\end{bmatrix}
]

Trung b√¨nh (N=6):
[
\frac{1}{6}
\begin{bmatrix}
-2 \ 0 \ 2
\end{bmatrix}
=============

\begin{bmatrix}
-0.3333 \ 0 \ 0.3333
\end{bmatrix}
]

Nh√¢n v·ªõi vector x trung b√¨nh (x-bar ‚âà 4.6667)

Do single-feature, gradient W = errors √ó xÃÑ:

[
\frac{\partial L}{\partial W}
=============================

\begin{bmatrix}
-0.3333\0\0.3333
\end{bmatrix} (4.6667)
======================

\begin{bmatrix}
-1.555\0\1.555
\end{bmatrix}
]

---

### C·∫≠p nh·∫≠t

[
W = 0 - 0.2
\begin{bmatrix}
-1.555\0\1.555
\end{bmatrix}
=============

\begin{bmatrix}
0.311\0\-0.311
\end{bmatrix}
]

[
b = 0 - 0.2
\begin{bmatrix}
-0.3333\0\0.3333
\end{bmatrix}
=============

\begin{bmatrix}
0.0666\0\-0.0666
\end{bmatrix}
]

---

# PH·∫¶N 3 ‚Äî CODE HO√ÄN CH·ªàNH

T√¥i ƒë∆∞a 4 phi√™n b·∫£n:

---

# ‚≠ê **1. ONE-SAMPLE ‚Äì 1 BI·∫æN**

```python
import numpy as np

X = np.array([1.0,2.5,4.0,5.5,7.0,8.0])
y = np.array([0,0,1,1,2,2])  # labels 0..2

def one_hot(y, K=3):
    out = np.zeros((len(y), K))
    out[np.arange(len(y)), y] = 1
    return out

Y = one_hot(y)

W = np.zeros((3,1))
b = np.zeros((3,1))

eta = 0.2
epochs = 2

for ep in range(epochs):
    for i in range(len(X)):
        x = np.array([[X[i]]])      # shape (1,1)
        t = Y[i].reshape(3,1)       # target
        
        z = W @ x + b
        y_hat = np.exp(z) / np.sum(np.exp(z))
        
        grad = (y_hat - t)
        W -= eta * grad * x
        b -= eta * grad

print("W =", W)
print("b =", b)
```

---

# ‚≠ê **2. FULL-SAMPLE ‚Äì 1 BI·∫æN**

```python
import numpy as np

X = np.array([1.0,2.5,4.0,5.5,7.0,8.0]).reshape(-1,1)
y = np.array([0,0,1,1,2,2])

def one_hot(y, K=3):
    out = np.zeros((len(y), K))
    out[np.arange(len(y)), y] = 1
    return out

Y = one_hot(y)

W = np.zeros((3,1))
b = np.zeros((3,1))

eta = 0.2
epochs = 2

for ep in range(epochs):
    z = X @ W.T + b.T      # shape (6,3)
    y_hat = np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)

    grad_W = ((y_hat - Y).T @ X) / len(X)
    grad_b = np.mean((y_hat - Y), axis=0, keepdims=True).T

    W -= eta * grad_W
    b -= eta * grad_b

print("W =", W)
print("b =", b)
```

---

# ‚≠ê **3. ONE-SAMPLE ‚Äì NHI·ªÄU BI·∫æN (multi-feature)**

```python
# X shape (N, D)
# W shape (K, D)
# b shape (K,1)

for ep in range(epochs):
    for i in range(N):
        x = X[i].reshape(D,1)
        t = Y[i].reshape(K,1)

        z = W @ x + b
        y_hat = np.exp(z)/np.sum(np.exp(z))

        grad = (y_hat - t)
        W -= eta * grad @ x.T
        b -= eta * grad
```

---

# ‚≠ê **4. FULL-SAMPLE ‚Äì NHI·ªÄU BI·∫æN**

```python
z = X @ W.T + b.T
y_hat = softmax(z)

grad_W = (y_hat - Y).T @ X / N
grad_b = np.mean(y_hat - Y, axis=0, keepdims=True).T
```

x√©t b√†i to√°n v√≠ d·ª•: d·ª± ƒëo√°n gi·ªëng hoa 0,1,2 d·ª±a tr√™n chi·ªÅu r·ªông c√°nh hoa.ch√∫ng ta s·∫Ω l·∫•y ra 6 ƒëi·ªÉm d·ªØ li·ªáu(N=6) ƒë·ªÉ th·ª±c hi·ªán t√≠nh to√°n 
m·ª•c ti√™u: 
1. th·ª±c hi·ªán c·∫≠p nh·∫•t W v√† b theo one-sample
2. th·ª±c hi·ªán c·∫≠p nh·∫≠t W v√† b theo full-sample

g·ª£i √Ω Ta c√≥ 3 l·ªõp ‚Üí W k√≠ch th∆∞·ªõc 3√ó1, b k√≠ch th∆∞·ªõc 3√ó1

[
W =
\begin{bmatrix}
0\ 0\ 0
\end{bmatrix},
\quad
b =
\begin{bmatrix}
0\ 0\ 0
\end{bmatrix}
]

Learning rate:
[
\eta = 0.2
]
S·ªë epoch:
[
n_epoch = 2
]



[
\frac{\partial L}{\partial W}
= \frac{1}{N}\sum_{i=1}^N (\hat{y}_i-y_i)x_i^T
]

[
\frac{\partial L}{\partial b}
= \frac{1}{N}\sum_{i=1}^N (\hat{y}_i-y_i)
]

---

### ‚ö†Ô∏è V√¨ ban ƒë·∫ßu W=0, b=0 n√™n t·∫•t c·∫£ m·∫´u ƒë·ªÅu cho c√πng softmax:

[
\hat{y}_i = [1/3,1/3,1/3]
]

### T√≠nh ( \hat{y}-y ) cho t·ª´ng m·∫´u, trung b√¨nh l·∫°i

[
\sum(\hat{y}-y) =
\begin{bmatrix}
-2 \ -0 \ +2
\end{bmatrix}
]

Trung b√¨nh (N=6):
[
\frac{1}{6}
\begin{bmatrix}
-2 \ 0 \ 2
\end{bmatrix}
=============

\begin{bmatrix}
-0.3333 \ 0 \ 0.3333
\end{bmatrix}
]

Nh√¢n v·ªõi vector x trung b√¨nh (x-bar ‚âà 4.6667)

Do single-feature, gradient W = errors √ó xÃÑ:

[
\frac{\partial L}{\partial W}
=============================

\begin{bmatrix}
-0.3333\0\0.3333
\end{bmatrix} (4.6667)
======================

\begin{bmatrix}
-1.555\0\1.555
\end{bmatrix}
]

---

### C·∫≠p nh·∫≠t

[
W = 0 - 0.2
\begin{bmatrix}
-1.555\0\1.555
\end{bmatrix}
=============

\begin{bmatrix}
0.311\0\-0.311
\end{bmatrix}
]

[
b = 0 - 0.2
\begin{bmatrix}
-0.3333\0\0.3333
\end{bmatrix}
=============

\begin{bmatrix}
0.0666\0\-0.0666
\end{bmatrix}
]



ƒê∆∞·ª£c ‚Äî m√¨nh ƒë√£ l√†m **to√†n b·ªô ph√©p t√≠nh theo Gaussian Naive Bayes (ƒëa bi·∫øn: height + weight)** cho d·ªØ li·ªáu b·∫°n cho, v√† **k·∫øt lu·∫≠n** r√µ r√†ng. D∆∞·ªõi ƒë√¢y l√† quy tr√¨nh + s·ªë th·ª±c chi ti·∫øt v√† gi·∫£i th√≠ch t·∫°i sao k·∫øt qu·∫£ nh∆∞ v·∫≠y.

---

## 1) D·ªØ li·ªáu (training)

```
Height  Weight   Class
160     50       G·∫ßy
159     49       G·∫ßy
162     52       B√¨nh th∆∞·ªùng
161     51       B√¨nh th∆∞·ªùng
172     72       B√¨nh th∆∞·ªùng
180     85       M·∫≠p
182     86       M·∫≠p
170     70       M·∫≠p
171     71       M·∫≠p
181     87       M·∫≠p
```

(T·ªïng N = 10 m·∫´u). M·∫´u c·∫ßn d·ª± ƒëo√°n: **x = (Height=169, Weight=69)**.

---

## 2) B∆∞·ªõc ‚Äì c√¥ng th·ª©c d√πng

* Prior: (P(c) = N_c/N).
* V·ªõi m·ªói feature gi·∫£ s·ª≠ Gaussian:
  (P(x_j \mid c) = \dfrac{1}{\sqrt{2\pi\sigma_{c,j}^2}} \exp!\big(-\dfrac{(x_j-\mu_{c,j})^2}{2\sigma_{c,j}^2}\big)).
* Do gi·∫£ thi·∫øt Naive Bayes ƒë·ªôc l·∫≠p t√≠nh t√≠ch likelihood theo t·ª´ng feature:
  (P(x \mid c) = \prod_j P(x_j \mid c)).
* Posterior ch∆∞a chu·∫©n h√≥a: (P(c)\cdot P(x\mid c)). Ch·ªçn l·ªõp c√≥ gi√° tr·ªã l·ªõn nh·∫•t (b·∫°n c√≥ th·ªÉ chu·∫©n ho√° ƒë·ªÉ th√†nh x√°c su·∫•t).

M√¨nh d√πng ph∆∞∆°ng ph√°p ML ƒë·ªÉ t√≠nh variance (chia cho (N_c), ddof=0).

---

## 3) Th·ªëng k√™ per-class (t√≠nh t·ª´ d·ªØ li·ªáu)

**S·ªë l∆∞·ª£ng / prior:**

* B√¨nh th∆∞·ªùng: (N=3) ‚Üí prior = 3/10 = 0.3
* G·∫ßy: (N=2) ‚Üí prior = 2/10 = 0.2
* M·∫≠p: (N=5) ‚Üí prior = 5/10 = 0.5

**Means & variances (t·ª´ng feature):**

| Class       | Œº_height | var_height | Œº_weight | var_weight |
| ----------- | -------: | ---------: | -------: | ---------: |
| B√¨nh th∆∞·ªùng | 165.0000 | 24.6666667 | 58.33333 | 93.5555556 |
| G·∫ßy         | 159.5000 |  0.2500000 | 49.50000 |  0.2500000 |
| M·∫≠p         | 176.8000 | 26.9600000 | 79.80000 | 58.1600000 |

(ƒê√¢y l√† c√°c gi√° tr·ªã trung b√¨nh v√† ph∆∞∆°ng sai t√≠nh t·ª´ m·∫´u m·ªói l·ªõp.)

---

## 4) T√≠nh likelihood cho x = (169,69)

T√≠nh Gaussian cho m·ªói feature+j k·∫øt h·ª£p:

* **B√¨nh th∆∞·ªùng**

  * (p(\text{height}=169\mid \text{B√¨nh}) \approx 0.0580767853)
  * (p(\text{weight}=69\mid \text{B√¨nh}) \approx 0.0224538708)
  * likelihood = 0.0580768 √ó 0.0224539 ‚âà **0.0013040486**
  * posterior (unnorm) = prior √ó likelihood = 0.3 √ó 0.0013040486 ‚âà 0.0003912146

* **G·∫ßy**

  * vars r·∫•t nh·ªè (0.25) v√† gi√° tr·ªã x c√°ch mean kh√° xa ‚Üí gaussian g·∫ßn 0:
  * (p(\text{height}=169\mid \text{G·∫ßy}) \approx 0.0) (r·∫•t g·∫ßn 0)
  * (p(\text{weight}=69\mid \text{G·∫ßy}) \approx 0.0)
  * likelihood ‚âà 0 ‚Üí posterior ‚âà 0

* **M·∫≠p**

  * (p(\text{height}=169\mid \text{M·∫≠p}) \approx 0.0248610252)
  * (p(\text{weight}=69\mid \text{M·∫≠p}) \approx 0.0191914870)
  * likelihood ‚âà 0.0004771200
  * posterior (unnorm) = 0.5 √ó 0.00047712 ‚âà 0.00023856

---

## 5) Chu·∫©n ho√° posterior ƒë·ªÉ d·ªÖ so s√°nh (t·ªïng = 1)

* B√¨nh th∆∞·ªùng: ‚âà **0.62120**
* G·∫ßy: ‚âà **0.00000**
* M·∫≠p: ‚âà **0.37880**

(Ph·∫ßn t√≠nh chi ti·∫øt m√¨nh ƒë√£ t√≠nh b·∫±ng code ƒë·ªÉ tr√°nh sai s·ªë l√†m tay.)

---

## 6) K·∫øt lu·∫≠n (d·ª± ƒëo√°n)

**D·ª± ƒëo√°n cho x = (169,69):** ‚Üí **B√¨nh th∆∞·ªùng** (v√¨ posterior l·ªõn nh·∫•t ‚âà 62.1%).

---

## 7) Gi·∫£i th√≠ch tr·ª±c quan v√¨ sao kh√¥ng ph·∫£i ‚ÄúG·∫ßy‚Äù hay ‚ÄúM·∫≠p‚Äù

* **G·∫ßy:** c√≥ variance r·∫•t nh·ªè (m·∫´u G·∫ßy c√≥ height 160,159 v√† weight 50,49) ‚Üí ph√¢n ph·ªëi r·∫•t h·∫πp. ƒêi·ªÉm (169,69) c√°ch mean G·∫ßy qu√° xa ‚Üí gaussian ~0 ‚Üí x√°c su·∫•t g·∫ßn 0.
* **M·∫≠p:** priors l·ªõn (0.5) nh∆∞ng mean height ~176.8, weight ~79.8 ‚Üí x h∆°i th·∫•p h∆°n => likelihood kh√¥ng qu√° cao.
* **B√¨nh th∆∞·ªùng:** mean height 165 v√† weight ~58 ‚Üí x=(169,69) t∆∞∆°ng ƒë·ªëi g·∫ßn c·∫£ 2 chi·ªÅu, khi nh√¢n 2 pdf cho likelihood t·ªët h∆°n so v·ªõi M·∫≠p ‚Üí c·ªông v·ªõi prior (0.3) cho posterior cao nh·∫•t.

---



T·ªët ‚Äî m√¨nh s·∫Ω xu·∫•t **b·∫£ng t√≠nh chi ti·∫øt t·ª´ng b∆∞·ªõc** (v·ªõi s·ªë h·ªçc r√µ r√†ng) ƒë·ªÉ b·∫°n ƒë·ªëi chi·∫øu. M√¨nh gi·ªØ nhi·ªÅu ch·ªØ s·ªë th·∫≠p ph√¢n ƒë·ªÉ th·∫•y ch√≠nh x√°c t·ª´ng trung gian.

---

## D·ªØ li·ªáu (training) ‚Äî t√≥m t·∫Øt

N = 10 m·∫´u

| Class       | m·∫´u (Height,Weight)                          | N_c |
| ----------- | -------------------------------------------- | --- |
| G·∫ßy         | (160,50), (159,49)                           | 2   |
| B√¨nh th∆∞·ªùng | (162,52), (161,51), (172,72)                 | 3   |
| M·∫≠p         | (180,85),(182,86),(170,70),(171,71),(181,87) | 5   |

Sample c·∫ßn d·ª± ƒëo√°n: (x=(\text{Height}=169,\ \text{Weight}=69)).

---

## B∆∞·ªõc 0 ‚Äî Priors

[
P(\text{B√¨nh}) = 3/10 = 0.3,\quad P(\text{G·∫ßy})=2/10=0.2,\quad P(\text{M·∫≠p})=5/10=0.5
]

---

## B∆∞·ªõc 1 ‚Äî Means v√† variances (ML, chia cho (N_c))

T√≠nh nhanh (k·∫øt qu·∫£):

* **B√¨nh th∆∞·ªùng**
  (\mu_h = 165.0000000000,; \sigma_h^2 = 24.6666666667)
  (\mu_w = 58.3333333333,; \sigma_w^2 = 93.5555555556)

* **G·∫ßy**
  (\mu_h = 159.5000000000,; \sigma_h^2 = 0.2500000000)
  (\mu_w = 49.5000000000,; \sigma_w^2 = 0.2500000000)

* **M·∫≠p**
  (\mu_h = 176.8000000000,; \sigma_h^2 = 26.9600000000)
  (\mu_w = 79.8000000000,; \sigma_w^2 = 58.1600000000)

(ƒê√¢y l√† mean v√† variance t√≠nh t·ª´ d·ªØ li·ªáu ƒë√£ cho.)

---

## B∆∞·ªõc 2 ‚Äî C√¥ng th·ª©c Gaussian (1D)

[
p(x\mid \mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp!\Big(-\frac{(x-\mu)^2}{2\sigma^2}\Big)
]

Ta √°p d·ª•ng cho c·∫£ hai feature (height, weight) r·ªìi nh√¢n (Naive Bayes gi·∫£ s·ª≠ ƒë·ªôc l·∫≠p).

---

## B∆∞·ªõc 3 ‚Äî T√≠nh t·ª´ng PDF (t·ª´ng class, t·ª´ng feature)

M√¨nh s·∫Ω tr√¨nh b√†y t·ª´ng l·ªõp, v·ªõi ph√©p thay s·ªë:

### A) Class = **B√¨nh th∆∞·ªùng**

**Height:**

* (x_h=169,; \mu_h=165,; \sigma_h^2=24.6666666667)
* (2\sigma^2 = 49.3333333334)
* ((x-\mu)^2 = (169-165)^2 = 16)
* exponent (= -16/49.3333333334 = -0.3248015)
* (\exp(\cdot)=0.7226797)
* denom (=\sqrt{2\pi\sigma^2}=\sqrt{2\pi\cdot24.6666666667}=\sqrt{154.99999999}=12.449895)
* (p_{\text{height}} = 0.7226797 / 12.449895 = \mathbf{0.0580767853})

**Weight:**

* (x_w=69,; \mu_w=58.3333333333,; \sigma_w^2=93.5555555556)
* ((x-\mu)^2 = (69-58.3333333333)^2 = 10.6666666667^2 = 113.7777777778)
* (2\sigma^2 = 187.1111111112)
* exponent (= -113.77777778/187.1111111112 = -0.6079171)
* (\exp(\cdot)=0.5445283)
* denom (=\sqrt{2\pi\cdot93.555555556}=24.248737)
* (p_{\text{weight}} = 0.5445283 / 24.248737 = \mathbf{0.0224538708})

**Likelihood (height √ó weight):**
[
\text{lik} = 0.0580767853 \times 0.0224538708 = \mathbf{0.0013040486}
]

**Posterior (unnormalized):**
[
P(\text{B√¨nh})\cdot \text{lik} = 0.3 \times 0.0013040486 = \mathbf{0.0003912146}
]

---

### B) Class = **G·∫ßy**

**Height:**

* (x_h=169,; \mu_h=159.5,; \sigma_h^2=0.25)
* ((x-\mu)^2 = (169-159.5)^2 = 9.5^2 = 90.25)
* (2\sigma^2 = 0.5)
* exponent (= -90.25 / 0.5 = -180.5)
* (\exp(-180.5)) l√† c·ª±c k√¨ nh·ªè ‚âà (1.45\times10^{-79})
* denom (=\sqrt{2\pi\cdot0.25}=\sqrt{1.5707963268}=1.252314)
* (p_{\text{height}} \approx 1.45\times10^{-79} / 1.252314 \approx \mathbf{1.16\times10^{-79}}) (‚âà 0 trong t√≠nh th·ª±c t·∫ø)

**Weight:**

* (x_w=69,; \mu_w=49.5,; \sigma_w^2=0.25)
* ((x-\mu)^2 = 19.5^2 = 380.25)
* exponent (= -380.25/0.5 = -760.5)
* (\exp(-760.5)) ‚âà extremely tiny (~(10^{-330}))
* (p_{\text{weight}} \approx \mathbf{0}) (v√¥ c√πng nh·ªè; th·ª±c t·∫ø l√†m tr√≤n v·ªÅ 0)

**Likelihood ‚âà 0** (s·∫£n ph·∫©m c·ªßa hai s·ªë ~0)

**Posterior (unnorm) ‚âà 0**

> K·∫øt lu·∫≠n: v√¨ ph∆∞∆°ng sai r·∫•t nh·ªè (l·ªõp G·∫ßy c√≥ 2 m·∫´u g·∫ßn nhau), ƒëi·ªÉm (169,69) qu√° xa mean ‚Üí probability ‚âà 0.

---

### C) Class = **M·∫≠p**

**Height:**

* (x_h=169,; \mu_h=176.8,; \sigma_h^2=26.96)
* ((x-\mu)^2 = (169-176.8)^2 = (-7.8)^2 = 60.84)
* (2\sigma^2 = 53.92)
* exponent (= -60.84/53.92 = -1.128353)
* (\exp(\cdot)=0.323764)
* denom (=\sqrt{2\pi\cdot26.96}=\sqrt{169.4528}=13.017401)
* (p_{\text{height}} = 0.323764 / 13.017401 = \mathbf{0.0248610252})

**Weight:**

* (x_w=69,; \mu_w=79.8,; \sigma_w^2=58.16)
* ((x-\mu)^2 = (69-79.8)^2 = (-10.8)^2 =116.64)
* (2\sigma^2 = 116.32)
* exponent (= -116.64/116.32 = -1.002754)
* (\exp(\cdot)=0.367415)
* denom (=\sqrt{2\pi\cdot58.16}=\sqrt{365.99904}=19.157012)
* (p_{\text{weight}} = 0.367415 / 19.157012 = \mathbf{0.0191914870})

**Likelihood:**
[
\text{lik} = 0.0248610252 \times 0.0191914870 = \mathbf{0.0004771200}
]

**Posterior (unnorm):**
[
P(\text{M·∫≠p})\cdot \text{lik} = 0.5 \times 0.0004771200 = \mathbf{0.0002385600}
]

---

## B∆∞·ªõc 4 ‚Äî Chu·∫©n ho√° posterior (t·ªïng = 1)

T·ªïng posterior (unnorm) =
(0.0003912146 + 0 + 0.0002385600 = 0.0006297746)

Chu·∫©n ho√°:

* (P(\text{B√¨nh}\mid x) = 0.0003912146 / 0.0006297746 = \mathbf{0.6211977791})
* (P(\text{G·∫ßy}\mid x) \approx \mathbf{0.0000000000})
* (P(\text{M·∫≠p}\mid x) = 0.0002385600 / 0.0006297746 = \mathbf{0.3788022209})

---

## B·∫£ng t·ªïng h·ª£p (r√∫t g·ªçn)

| Class       | Prior  |    p(height) |    p(weight) |   likelihood | posterior_unnorm | posterior_norm |
| ----------- | ------ | -----------: | -----------: | -----------: | ---------------: | -------------: |
| B√¨nh th∆∞·ªùng | 0.3000 | 0.0580767853 | 0.0224538708 | 0.0013040486 |     0.0003912146 |   0.6211977791 |
| G·∫ßy         | 0.2000 |    ‚âà1.16e-79 | ‚âà0 (‚âà1e-330) |           ‚âà0 |               ‚âà0 |   0.0000000000 |
| M·∫≠p         | 0.5000 | 0.0248610252 | 0.0191914870 | 0.0004771200 |     0.0002385600 |   0.3788022209 |

(Tr√™n th·ª±c t·∫ø ta coi m·ªçi gi√° tr·ªã < ~1e-16 l√† 0 khi hi·ªÉn th·ªã; l·ªõp G·∫ßy g·∫ßn nh∆∞ 0.)

---

## K·∫øt lu·∫≠n

* **D·ª± ƒëo√°n cho (169,69)** theo Gaussian Naive Bayes (v·ªõi c√°c ph√©p t√≠nh tr√™n): **B√¨nh th∆∞·ªùng** (posterior ‚âà 0.6212).
* M·∫≠p c√≥ posterior ‚âà 0.3788, c√≤n G·∫ßy g·∫ßn 0.

