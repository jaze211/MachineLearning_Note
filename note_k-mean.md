# K-means trong h·ªçc m√°y ‚Äî gi·∫£i th√≠ch chi ti·∫øt v√† c√°c ph·∫ßn li√™n quan

K-means l√† m·ªôt thu·∫≠t to√°n **ph√¢n c·ª•m (clustering)** kh√¥ng gi√°m s√°t r·∫•t ph·ªï bi·∫øn. M·ª•c ti√™u c·ªßa K-means l√† **chia N m·∫´u th√†nh K c·ª•m** sao cho c√°c m·∫´u c√πng c·ª•m c√†ng gi·ªëng nhau (v·ªÅ kho·∫£ng c√°ch t·ªõi t√¢m c·ª•m) c√†ng t·ªët.

---

## 1. M·ª•c ti√™u to√°n h·ªçc (objective)

G·ªçi (X={x_1,\dots,x_N}), (x_i\in\mathbb{R}^d). K-means t√¨m c√°c centroid ({\mu_1,\dots,\mu_K}) v√† ph√¢n c·ª•m (C_1,\dots,C_K) b·∫±ng c√°ch t·ªëi thi·ªÉu h√†m m·∫•t m√°t:

[
\min_{C,\mu} ; J = \sum_{k=1}^K \sum_{x_i \in C_k} |x_i - \mu_k|_2^2
]

ƒê√¢y l√† t·ªïng ph∆∞∆°ng sai trong c√°c c·ª•m (within-cluster sum of squares).

---

## 2. Thu·∫≠t to√°n (Lloyd‚Äôs algorithm ‚Äî ph·ªï bi·∫øn)

L·∫∑p cho ƒë·∫øn khi h·ªôi t·ª•:

1. **G√°n nh√£n (Assignment step)**: v·ªõi m·ªói ƒëi·ªÉm (x_i), g√°n v√†o c·ª•m g·∫ßn nh·∫•t:
   [
   c_i = \arg\min_{k} |x_i - \mu_k|^2
   ]
2. **C·∫≠p nh·∫≠t centroid (Update step)**: v·ªõi m·ªói c·ª•m k, t√≠nh trung b√¨nh m·∫´u trong c·ª•m:
   [
   \mu_k = \frac{1}{|C_k|}\sum_{x_i\in C_k} x_i
   ]
3. D·ª´ng khi nh√£n kh√¥ng thay ƒë·ªïi ho·∫∑c khi gi·∫£m J r·∫•t nh·ªè.

Thu·∫≠t to√°n lu√¥n gi·∫£m ho·∫∑c gi·ªØ nguy√™n gi√° tr·ªã J, n√™n h·ªôi t·ª• v·ªÅ m·ªôt ƒëi·ªÉm c·ª±c tr·ªã (c·ª•c b·ªô), nh∆∞ng kh√¥ng ƒë·∫£m b·∫£o c·ª±c tr·ªã to√†n c·ª•c.

---

## 3. Kho·∫£ng c√°ch & kh√¥ng gian ƒë·∫∑c tr∆∞ng

* K-means m·∫∑c ƒë·ªãnh d√πng **kho·∫£ng c√°ch Euclid (L2)**. ƒêi·ªÅu n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi gi·∫£ ƒë·ªãnh r·∫±ng c·ª•m c√≥ h√¨nh c·∫ßu trong kh√¥ng gian ƒë·∫∑c tr∆∞ng.
* Do v·∫≠y **scale c·ªßa c√°c feature r·∫•t quan tr·ªçng** ‚Üí lu√¥n c√¢n nh·∫Øc chu·∫©n h√≥a (StandardScaler, MinMaxScaler) tr∆∞·ªõc khi ch·∫°y K-means.

---

## 4. Kh·ªüi t·∫°o centroid v√† k-means++

* Kh·ªüi t·∫°o ng·∫´u nhi√™n centroid c√≥ th·ªÉ d·∫´n t·ªõi nghi·ªám k√©m (local minima).
* **k-means++** l√† ph∆∞∆°ng ph√°p kh·ªüi t·∫°o ph·ªï bi·∫øn: ch·ªçn centroid ƒë·∫ßu ti√™n ng·∫´u nhi√™n, c√°c centroid ti·∫øp theo v·ªõi x√°c su·∫•t t·ªâ l·ªá kho·∫£ng c√°ch¬≤ t·ªõi centroid g·∫ßn nh·∫•t. k-means++ c·∫£i thi·ªán t√≠nh ·ªïn ƒë·ªãnh v√† th∆∞·ªùng h·ªôi t·ª• nhanh h∆°n.

---

## 5. V·∫•n ƒë·ªÅ ch·ªçn K (s·ªë c·ª•m)

M·ªôt s·ªë k·ªπ thu·∫≠t:

* **Elbow method**: v·∫Ω t·ªïng within-cluster SSE (J) theo K; ch·ªçn K t·∫°i ‚Äúkhu·ª∑u‚Äù (ƒëi·ªÉm gi·∫£m l·ª£i √≠ch b·∫Øt ƒë·∫ßu gi·∫£m).
* **Silhouette score**: cho m·ªói ƒëi·ªÉm (s_i=(b_i-a_i)/\max(a_i,b_i)) v·ªõi (a_i)=avg dist t·ªõi c·ª•m m√¨nh, (b_i)=min avg dist t·ªõi c·ª•m kh√°c. Trung b√¨nh silhouette g·∫ßn 1 t·ªët, g·∫ßn 0 ranh gi·ªõi, √¢m l√† sai.
* **Gap statistic**, **BIC/AIC (v·ªõi m√¥ h√¨nh h·ªón h·ª£p Gaussian)**, ho·∫∑c domain knowledge.

---

## 6. ƒê·ªô ph·ª©c t·∫°p v√† hi·ªáu nƒÉng

* M·ªói v√≤ng l·∫∑p assignment: (O(N K d)) (t√≠nh kho·∫£ng c√°ch N√óK), update: (O(N d)). T·ªïng: (O(N K d \cdot I)) v·ªõi I s·ªë v√≤ng l·∫∑p.
* **Mini-batch K-means** gi·∫£m chi ph√≠ cho d·ªØ li·ªáu l·ªõn b·∫±ng c√°ch c·∫≠p nh·∫≠t b·∫±ng m·∫´u ng·∫´u nhi√™n nh·ªè (scikit-learn cung c·∫•p).

---

## 7. H·∫°n ch·∫ø & l∆∞u √Ω th·ª±c t·∫ø

* **Ch·ªâ ho·∫°t ƒë·ªông t·ªët khi c·ª•m d·∫°ng h√¨nh c·∫ßu** ‚Äî kh√¥ng t·ªët v·ªõi c·ª•m c√≥ h√¨nh d·∫°ng ph·ª©c t·∫°p, m·∫≠t ƒë·ªô kh√°c nhau, ho·∫∑c d·ªØ li·ªáu c√≥ nhi·ªÖu/outliers.
* **Nh·∫°y v·ªõi outliers** (outlier k√©o centroid).
* **Y√™u c·∫ßu K c·ªë ƒë·ªãnh**; n·∫øu kh√¥ng bi·∫øt K, d√πng k·ªπ thu·∫≠t ch·ªçn K.
* **Kh√¥ng x·ª≠ l√Ω t·ªët d·ªØ li·ªáu categorical** (ph·∫£i transform: one-hot, target encoding, ho·∫∑c d√πng k-modes / k-prototypes).
* **K·∫øt qu·∫£ ph·ª• thu·ªôc kh·ªüi t·∫°o** ‚Üí d√πng nhi·ªÅu l·∫ßn v·ªõi random_state kh√°c nhau v√† ch·ªçn k·∫øt qu·∫£ t·ªët nh·∫•t.

---

## 8. C√°c bi·∫øn th·ªÉ & thu·∫≠t to√°n li√™n quan

* **Mini-batch K-means**: cho d·ªØ li·ªáu l·ªõn, c·∫≠p nh·∫≠t theo l√¥ nh·ªè.
* **K-medoids (PAM)**: d√πng medoid (m·∫´u th·ª±c) thay v√¨ centroid, robust v·ªõi outliers.
* **K-modes / k-prototypes**: cho d·ªØ li·ªáu categorical ho·∫∑c h·ªón h·ª£p.
* **Gaussian Mixture Models (GMM)**: m√¥ ph·ªèng m·ªói c·ª•m b·∫±ng Gaussian ‚Üí soft assignment (EM algorithm).
* **Spectral clustering, DBSCAN, Hierarchical clustering**: khi d·ªØ li·ªáu kh√¥ng suit K-means (non-convex shapes, varying density).
* **Bisecting K-means**: hierarchical variant.

---

## 9. ƒê√°nh gi√° k·∫øt qu·∫£ clustering

Kh√¥ng c√≥ nh√£n ‚Üí d√πng metrics kh√¥ng gi√°m s√°t:

* **Within-cluster sum of squares (SSE)** ‚Äî nh·ªè t·ªët.
* **Silhouette score** ‚Äî gi√° tr·ªã ‚àà [‚àí1,1].
* **Davies-Bouldin index**, **Calinski-Harabasz index**.
  N·∫øu c√≥ nh√£n ground truth: **Adjusted Rand Index (ARI)**, **Normalized Mutual Information (NMI)**.

---

## 10. Checklist th·ª±c h√†nh tr∆∞·ªõc khi ch·∫°y K-means

* Chu·∫©n h√≥a d·ªØ li·ªáu (StandardScaler / MinMax) n·∫øu feature kh√°c scale.
* Lo·∫°i/ x·ª≠ l√Ω outliers n·∫øu c·∫ßn.
* Ch·ªçn s·ªë c·ª•m K (elbow / silhouette / domain).
* D√πng k-means++ kh·ªüi t·∫°o.
* Th·ª±c hi·ªán nhi·ªÅu l·∫ßn (n_init) v√† ch·ªçn best inertia.
* Ki·ªÉm tra silhouette v√† visualize (PCA/TSNE projection).
* N·∫øu d·ªØ li·ªáu l·ªõn, c√¢n nh·∫Øc mini-batch K-means.

---

## 11. V√≠ d·ª• code (scikit-learn, Python)

```python
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

X = ...  # numpy array shape (N,d)

# scale
scaler = StandardScaler()
Xs = scaler.fit_transform(X)

# KMeans with k-means++ and multiple inits
k = 4
km = KMeans(n_clusters=k, init="k-means++", n_init=10, random_state=42)
km.fit(Xs)

labels = km.labels_
centroids = km.cluster_centers_
inertia = km.inertia_  # SSE

# silhouette
sil = silhouette_score(Xs, labels)
print("Inertia:", inertia, "Silhouette:", sil)
```

---

## 12. Khi n√†o d√πng K-means (use cases)

* Kh·∫£o s√°t d·ªØ li·ªáu (exploratory analysis)
* T√°ch nh√≥m kh√°ch h√†ng (customer segmentation)
* Ph√¢n c·ª•m ·∫£nh/ƒëi·ªÉm d·ªØ li·ªáu (preprocessing)
* Vector quantization, compression
* L√†m ti·ªÅn x·ª≠ l√Ω cho algorithms kh√°c (coarsening, prototype selection)

---

## 13. M·∫πo & th·ªß thu·∫≠t n√¢ng cao

* N·∫øu d·ªØ li·ªáu sparse (TF-IDF text), c√≥ th·ªÉ d√πng **KMeans tr√™n cosine distance** (b√¨nh th∆∞·ªùng scikit-learn ƒëi·ªÅu ch·ªânh b·∫±ng chu·∫©n ho√° l2 ‚Üí cosine similarity).
* V·ªõi TF-IDF, **b√¨nh th∆∞·ªùng kh√¥ng n√™n ti√™u chu·∫©n h√≥a b·∫±ng StandardScaler**; thay v√†o ƒë√≥, d√πng normalize (l2) ƒë·ªÉ t∆∞∆°ng th√≠ch cosine.
* Visualize clusters qua PCA/TSNE/UMAP ƒë·ªÉ ki·ªÉm tra c·∫•u tr√∫c.
* N·∫øu mu·ªën soft cluster assignments / probabilistic ‚Üí d√πng GMM.

---

### K·∫øt lu·∫≠n ng·∫Øn g·ªçn

K-means l√† c√¥ng c·ª• ph√¢n c·ª•m ƒë∆°n gi·∫£n, nhanh, ph√π h·ª£p khi c·ª•m g·∫ßn h√¨nh c·∫ßu v√† d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a. Quan tr·ªçng l√† **l·ª±a ch·ªçn K, kh·ªüi t·∫°o t·ªët (k-means++), chu·∫©n h√≥a** v√† c√¢n nh·∫Øc c√°c bi·∫øn th·ªÉ (mini-batch, k-medoids, GMM) n·∫øu d·ªØ li·ªáu c√≥ ƒë·∫∑c t√≠nh ph·ª©c t·∫°p h∆°n.

N·∫øu b·∫°n mu·ªën, m√¨nh s·∫Ω:

* Vi·∫øt **notebook minh ho·∫°** v·ªõi v√≠ d·ª• (synthetic + real data), elbow plot, silhouette, PCA visualization, v√† mini-batch;
* Ho·∫∑c chuy·ªÉn v√≠ d·ª• sang d·ªØ li·ªáu c·ªßa b·∫°n (upload m·ªôt sample) v√† ch·∫°y clustering c·ª• th·ªÉ.

B·∫°n mu·ªën m√¨nh l√†m ti·∫øp ph·∫ßn n√†o?

C·∫£m ∆°n b·∫°n ƒë√£ cung c·∫•p h√¨nh ·∫£nh! H√¨nh ·∫£nh n√†y ƒëang tr√¨nh b√†y m·ªôt v√≠ d·ª• **Th·ª±c h√†nh** c·ª• th·ªÉ v·ªÅ thu·∫≠t to√°n **K-Means Clustering** (Ph√¢n c·ª•m K-Means) tr√™n m·ªôt b·ªô d·ªØ li·ªáu nh·ªè.

ƒê√¢y l√† b·∫£n t√≥m t·∫Øt c√°c b∆∞·ªõc ƒëang ƒë∆∞·ª£c th·ª±c hi·ªán, d·ª±a tr√™n n·ªôi dung slide:

## üìä V√≠ d·ª• K-Means Clustering

B·ªô d·ªØ li·ªáu g·ªìm 6 sinh vi√™n (SV) v·ªõi 2 ƒë·∫∑c tr∆∞ng (feature): **ƒêi·ªÉm h·ªçc t·∫≠p** v√† **ƒêi·ªÉm r√®n luy·ªán**.

| SV | ƒêi·ªÉm h·ªçc t·∫≠p ($x_1$) | ƒêi·ªÉm r√®n luy·ªán ($x_2$) |
| :---: | :---: | :---: |
| **S01** | 85 | 83 |
| **S02** | 70 | 59 |
| **S03** | 90 | 50 |
| **S04** | 50 | 85 |
| **S05** | 50 | 50 |
| **S06** | 90 | 85 |

### üéØ B∆∞·ªõc 1: L·ª±a ch·ªçn $k$

* **Ch·ªçn $k=3$.** (T·ª©c l√† thu·∫≠t to√°n s·∫Ω chia d·ªØ li·ªáu th√†nh 3 c·ª•m).

### üöÄ B∆∞·ªõc 2: Kh·ªüi t·∫°o tr·ªçng t√¢m ban ƒë·∫ßu

C√°c tr·ªçng t√¢m (centroids) ban ƒë·∫ßu ($\mu_i^0$) ƒë∆∞·ª£c ch·ªçn **theo ph∆∞∆°ng ph√°p Forgy** (th∆∞·ªùng l√† ch·ªçn ng·∫´u nhi√™n $k$ ƒëi·ªÉm d·ªØ li·ªáu l√†m tr·ªçng t√¢m).

* $\mu_1^0 = (85, 83)$ (Tr√πng v·ªõi d·ªØ li·ªáu c·ªßa SV S01)
* $\mu_2^0 = (70, 59)$ (Tr√πng v·ªõi d·ªØ li·ªáu c·ªßa SV S02)
* $\mu_3^0 = (90, 50)$ (Tr√πng v·ªõi d·ªØ li·ªáu c·ªßa SV S03)

### üîó B∆∞·ªõc 3: G√°n ƒëi·ªÉm d·ªØ li·ªáu v√†o c·ª•m g·∫ßn nh·∫•t (V√≤ng l·∫∑p 1)

ƒê√¢y l√† b∆∞·ªõc t√≠nh to√°n **kho·∫£ng c√°ch (K/C)** t·ª´ m·ªói ƒëi·ªÉm d·ªØ li·ªáu ƒë·∫øn 3 tr·ªçng t√¢m $\mu_1^0, \mu_2^0, \mu_3^0$ v√† g√°n ƒëi·ªÉm ƒë√≥ v√†o tr·ªçng t√¢m g·∫ßn nh·∫•t. Kho·∫£ng c√°ch th∆∞·ªùng d√πng l√† **kho·∫£ng c√°ch Euclidean** ($d = \sqrt{(x_1 - \mu_1)^2 + (x_2 - \mu_2)^2}$).

K·∫øt qu·∫£ g√°n c·ª•m (theo slide) l√†:

* **C·ª•m 1 (g·∫ßn $\mu_1^0$):** S1, S6
    * *L∆∞u √Ω: S6(90, 85) g·∫ßn S1(85, 83) h∆°n l√† S3(90, 50) ho·∫∑c S2(70, 59).*
* **C·ª•m 2 (g·∫ßn $\mu_2^0$):** S2, S4, S5
    * *L∆∞u √Ω: S4(50, 85) v√† S5(50, 50) ƒë∆∞·ª£c g√°n v√†o C·ª•m 2 (g·∫ßn S2(70, 59)).*
* **C·ª•m 3 (g·∫ßn $\mu_3^0$):** S3
    * *L∆∞u √Ω: Ch·ªâ c√≥ S3(90, 50) ·ªü c·ª•m n√†y.*

---

## ‚è≠Ô∏è B∆∞·ªõc Ti·∫øp theo (Ch∆∞a c√≥ trong h√¨nh)

Sau B∆∞·ªõc 3, thu·∫≠t to√°n s·∫Ω chuy·ªÉn sang **B∆∞·ªõc 4: C·∫≠p nh·∫≠t tr·ªçng t√¢m m·ªõi ($\mu_i^1$)**.

**Y√™u c·∫ßu c·ªßa B∆∞·ªõc 4:** T√≠nh l·∫°i v·ªã tr√≠ tr·ªçng t√¢m m·ªõi b·∫±ng c√°ch l·∫•y **gi√° tr·ªã trung b√¨nh** c·ªßa t·∫•t c·∫£ c√°c ƒëi·ªÉm trong c·ª•m ƒë√≥.

* **$\mu_1^1$ (C·ª•m S1, S6):** $\mu_1^1 = \left( \frac{85+90}{2}, \frac{83+85}{2} \right) = \left( 87.5, 84 \right)$
* **$\mu_2^1$ (C·ª•m S2, S4, S5):** $\mu_2^1 = \left( \frac{70+50+50}{3}, \frac{59+85+50}{3} \right) = \left( \frac{170}{3}, \frac{194}{3} \right) \approx (56.67, 64.67)$
* **$\mu_3^1$ (C·ª•m S3):** $\mu_3^1 = (90, 50)$

Sau ƒë√≥, thu·∫≠t to√°n s·∫Ω **l·∫∑p l·∫°i** B∆∞·ªõc 3 (G√°n d·ªØ li·ªáu) v·ªõi c√°c tr·ªçng t√¢m m·ªõi ($\mu_1^1, \mu_2^1, \mu_3^1$) cho ƒë·∫øn khi kh√¥ng c√≤n s·ª± thay ƒë·ªïi n√†o trong vi·ªác g√°n c·ª•m.


b∆∞·ªõc 5 ki·ªÉm tra h·ªôi t·ª• 
     thu·∫≠t to√°n l·∫∑p l·∫°i 2 b∆∞·ªõc 3 v√† 4 cho t·ªõi khi tr·ªçng t√¢m kh√¥ng thay ƒë·ªïi nh·ªè h∆°n m·ªôt ng∆∞·ª°ng cho tr∆∞·ªõc